{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq2W8sTjJAHpejdzYkfNXB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p2Pj2bSFK1zb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "#load all the datasets\n",
        "\n",
        "import csv\n",
        "df1 = pd.read_csv('/content/USvideos.csv',header = None, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "df2 = pd.read_csv('/content/CAvideos.csv',header = None, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "df3 = pd.read_csv('/content/GBvideos.csv',header = None, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "\n",
        "#load the datasets containing the category names\n",
        "data1 = json.load(open('US_category_id.json'))\n",
        "data2 = json.load(open('CA_category_id.json'))\n",
        "data3 = json.load(open('GB_category_id.json'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract categories from data while handling KeyError\n",
        "def category_extractor(data):\n",
        "    categories = {}\n",
        "    for item in data.get('items', []):\n",
        "        try:\n",
        "            video_id = item['id']\n",
        "            snippet = item['snippet']\n",
        "            title = snippet['title']\n",
        "            categories[video_id] = title\n",
        "        except KeyError:\n",
        "            pass  # Handle the case where 'id' or 'snippet' keys are missing\n",
        "    return categories\n",
        "\n",
        "# Create a dictionary to hold all the data and category information\n",
        "data_dict = {\n",
        "    'data1': data1,\n",
        "    'data2': data2,\n",
        "    'data3': data3\n",
        "}\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Iterate through the data and create individual DataFrames\n",
        "for data_name, data in data_dict.items():\n",
        "    category_data = category_extractor(data)\n",
        "    df_temp = pd.DataFrame(category_data.items(), columns=['video_id', 'category_title'])\n",
        "    df_temp['source'] = data_name  # Add a source column to track data source\n",
        "    combined_df = combined_df.append(df_temp, ignore_index=True)\n",
        "\n",
        "# Drop duplicates based on video_id\n",
        "combined_df.drop_duplicates(subset='video_id', inplace=True)\n",
        "\n",
        "# Filter for Entertainment category\n",
        "entertainment_df = combined_df[combined_df['category_title'] == 'Entertainment']\n",
        "\n",
        "# Clean and create the corpus\n",
        "entertainment_df['cleaned_title'] = entertainment_df['category_title'].apply(clean_text)\n",
        "corpus = entertainment_df['cleaned_title'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npJ36LlY6LIz",
        "outputId": "f89888e6-dedb-4608-d044-844659c3e1f0"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-183-3bffa2857d0e>:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_df = combined_df.append(df_temp, ignore_index=True)\n",
            "<ipython-input-183-3bffa2857d0e>:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_df = combined_df.append(df_temp, ignore_index=True)\n",
            "<ipython-input-183-3bffa2857d0e>:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_df = combined_df.append(df_temp, ignore_index=True)\n",
            "<ipython-input-183-3bffa2857d0e>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  entertainment_df['cleaned_title'] = entertainment_df['category_title'].apply(clean_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Generating sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "  #get tokens\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "  #convert to sequence of tokens\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "   token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "  return input_sequences, total_words\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
      ],
      "metadata": {
        "id": "rE4B4krPh6k1"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##sequence length must be same so padding the sequence\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def generate_padded_sequences(input_sequences, total_words):\n",
        "    if not input_sequences:\n",
        "        raise ValueError(\"Input sequences is empty.\")\n",
        "\n",
        "    max_sequence_len = max(len(seq) for seq in input_sequences)\n",
        "\n",
        "    # Pad the input sequences\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "    # Split into predictors and labels\n",
        "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "    # Convert labels to one-hot encoded format\n",
        "    label = to_categorical(label, num_classes=total_words)\n",
        "\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "# Example usage:\n",
        "inp_sequences = [[1, 2, 3], [4, 5, 6, 7], [8, 9]]\n",
        "total_words = 100\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences, total_words)\n"
      ],
      "metadata": {
        "id": "WbmxPRTJjCXm"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Creating a LSTM layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.activations import softmax\n",
        "\n",
        "# Assuming you have already defined max_sequence_len, total_words, predictors, and label.\n",
        "\n",
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 100, input_length=input_len))\n",
        "\n",
        "    # LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.fit(predictors, label, epochs=20, verbose=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExCBNyrj1v1",
        "outputId": "1ad053aa-b239-47aa-e201-9a3bd779241a"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c54fa2cbaf0>"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len,tokenizer):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding='pre')\n",
        "    predicted_class= np.argmax(model.predict(token_list, verbose=0))\n",
        "    #predicted_probs = model.predict(token_list, verbose=0)\n",
        "    #predicted_class = np.argmax(predicted_probs)\n",
        "\n",
        "  output_word = ''\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == predicted_class:\n",
        "      output_word = word\n",
        "    break\n",
        "    seed_text += '' +output_word\n",
        "  return seed_text.title()"
      ],
      "metadata": {
        "id": "3bsgYglYyG-X"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text('spiderman', 5, model, max_sequence_len,tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Pzxl9Dyh8S",
        "outputId": "ebec4501-ba50-4a2f-bf00-b6f4386837ef"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spiderman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpbRtYDfy1lR"
      },
      "execution_count": 188,
      "outputs": []
    }
  ]
}